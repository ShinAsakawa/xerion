{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xerion_demo001.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/xerion/blob/master/notebooks/xerion_demo001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "x1K11lyO9_iR"
      },
      "cell_type": "markdown",
      "source": [
        "# Xerion, data management for simulating SM89 and PMSP96\n",
        "\n",
        "for [wbai handson](https://wba-initiative.org/3411/)\n",
        "\n",
        "<div>\n",
        "    <center><img src=\"https://wba-initiative.org/wp-content/uploads/2015/05/logo.png\" style=\"width:29%\"></center>\n",
        "</div>\n",
        "    \n",
        "\n",
        "<p style='text-align:center;'>\n",
        "    <font color='green' size='+1' style='font-weight:bold;'>Shin Asakawa &lt;asakawa@ieee.org&gt;</font>\n",
        "</p>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AjELIKpl9_iS"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 手順1. `Git clone` する"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fobgjckeHMI5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# numpy の不都合によりバージョンを変更します\n",
        "!pip install --user -U numpy==1.16.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8EzAFB-K6OVZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "- 上のセルを実行後，最後に表示される  `RESTART RUNTIME` ボタンをクリックしてください\n",
        "- その後下のセルの実行に進んでください"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ricQMThk9_iT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GitHub から必要なパッケージを入手してください\n",
        "!git clone https://github.com/ShinAsakawa/xerion.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fLuvTGrfF0SZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# インストール結果を確認します\n",
        "import numpy as np\n",
        "from xerion.xerion import Xerion\n",
        "\n",
        "dataset = Xerion()\n",
        "#dataset.descr()      # 行頭の # を外して本セルを実行すると xerion の諸元を表示します\n",
        "#dataset.usage()      # 行頭の # を外して本セルを実行すると簡単なサンプルコードを表示します"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7jxitKkW9_iX"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 手順2. データの読み込み"
      ]
    },
    {
      "metadata": {
        "id": "uwZDwh-q5_ZI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset.grapheme   # 入力単語を表示します\n",
        "dataset.phoneme    # 出力，単語の読みの ARPABET 表現です\n",
        "dataset.seq        # オリジナルのデータ系列番号です\n",
        "dataset.freq       # 単語の頻度情報です\n",
        "dataset.inputs     # 入力データの三連表現\n",
        "dataset.outputs    # 出力データの三連表現"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fOq3LKxg9_ig"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 手順3. 読み込んだデータの確認\n",
        "\n",
        "ここに `input`, `output`, `freq`, `grapheme`, `phoneme`, `seq`, `tag` でアクセスできるようにしてあります。\n",
        "\n",
        "- input: np.ndarrray((2998, 105), dtype=float32) # 文字単語のトリプレット表現 105 次元ベクトル\n",
        "- output: np.ndarray((2998, 61), dtype=float32)  # 対応する音韻トリプレット表現 61 次元ベクトル\n",
        "- frep: np.ndarray((2998,), dtype=float32)       # 対応する頻度情報\n",
        "- seq: \n",
        "- grapheme: 入力文字列リスト\n",
        "- phoneme: 出力音韻列リスト ARPABET 表現\n",
        "- tag: 無視してください\n",
        "\n",
        "SM89, PMSP96 では意味層は実装されていません。<br>\n",
        "以下に例を示します"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pF7mND3F9_ix",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#以下に 3 連表現を示します。PMSP96論文中では triplet と呼ばれていたオンセット onset，母音 vowel, コーダ coda です。\n",
        "for item in [dataset.Orthography, dataset.Phonology]:\n",
        "    for entry in ['onset', 'vowel', 'coda']:\n",
        "        print(item[entry])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Y9glST6r9_i0"
      },
      "cell_type": "markdown",
      "source": [
        "## 同形異音語について"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7a4QG-c99_i1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grapheme_set = set(dataset.grapheme)\n",
        "len(grapheme_set)\n",
        "\n",
        "print('データ総数:{0} とユニークなデータ数:{1} との差が同形異音語数になります'.format(\n",
        "    len(dataset.grapheme),len(grapheme_set)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l3Qgx_fz9_i4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 実際に同形異音語を表示してみましょう\n",
        "prev_word = \"\"\n",
        "n_homographs = 0\n",
        "for word in sorted(dataset.grapheme):\n",
        "    if word == prev_word:\n",
        "        n_homographs += 1\n",
        "        print('{0}:{1}'.format(n_homographs, word))\n",
        "    prev_word = word\n",
        "print('\\nn_homographs={}'.format(n_homographs)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JxGjLhNa9_jA"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# データないに登録された各単語の発音を調べる\n",
        "\n",
        "- pyhon で自然言語処理する際に使われる `nltk` を使って単語の発音を調べてみます。\n",
        "- `nltk` については [https://www.nltk.org/](https://www.nltk.org/) をご覧ください"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4xr1IdLx9_jA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Colaboratory では cmudict がダウンロードされいないようなのでダウンロードします\n",
        "# 一度だけ実行すればよいので，その都度実行する必要はありません\n",
        "import nltk\n",
        "nltk.download('cmudict')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ugdv9Mh09_jE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import cmudict\n",
        "arpabet = cmudict.dict()\n",
        "\n",
        "# 1. Xerion の単語を取り出します\n",
        "words = dataset.grapheme\n",
        "\n",
        "# 2. Xerion に存在する単語のうち cmudict に発音の登録のない単語を抜き出します。\n",
        "not_in_arpabet = [word for word in words if not word in arpabet]\n",
        "\n",
        "print('{0}/{1}={2:.2f} percent words in Xerion data were not registered in ARPABET.'\n",
        "      .format(len(not_in_arpabet), len(words), len(not_in_arpabet)/len(words)*100))\n",
        "\n",
        "# 随分ありますね。\n",
        "print(not_in_arpabet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yLdAaV8V9_jI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ちなみに cmu 辞書についてですが\n",
        "print(len(arpabet))\n",
        "\n",
        "arpabet_vocab = [v for v in arpabet.keys()]\n",
        "print(arpabet_vocab[:5])\n",
        "arpabet_sounds = [s for s in arpabet.values()]\n",
        "print(arpabet_sounds[:5])\n",
        "#上記の表記については ARPABET をご覧ください"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8aVOWZvK9_jL"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# ARPABET に登録されていない単語の処理"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LaWJstQK9_jN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import product as iterprod\n",
        "#arpabet = nltk.corpus.cmudict.dict()\n",
        "\n",
        "def wordbreak(s):\n",
        "    \"\"\"\n",
        "    See https://stackoverflow.com/questions/33666557/get-phonemes-from-any-word-in-python-nltk-or-other-modules\n",
        "\n",
        "    \"\"\"\n",
        "    s = s.lower()\n",
        "    if s in arpabet:\n",
        "        return arpabet[s]\n",
        "    middle = len(s)/2\n",
        "    partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
        "    for i in partition:\n",
        "        pre, suf = (s[:i], s[i:])\n",
        "        if pre in arpabet and wordbreak(suf) is not None:\n",
        "            return [x+y for x,y in iterprod(arpabet[pre], wordbreak(suf))]\n",
        "    return None\n",
        "\n",
        "# nltk の cmudict に存在しない単語の読みを表示\n",
        "for word in not_in_arpabet:\n",
        "    print(word, wordbreak(word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_92WfwDD9_jQ"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 意味層の実装\n",
        "\n",
        "from <https://fasttext.cc/docs/en/pretrained-vectors.html>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HBy5GPFt9_jQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fastText データの読み込み。次の行頭の # を外してください\n",
        "#!time wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IWQUyxPA9_jS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word2vec_file = 'wiki.en.vec'\n",
        "with open(word2vec_file, 'r') as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "semantics_wiki = {}\n",
        "word_list = list(dataset.grapheme)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q23_ttQB9_jX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "for line in lines:    \n",
        "    buff = line.strip().split(' ')\n",
        "    word = buff[0]\n",
        "    if word in word_list:\n",
        "        semantics_wiki[word] = np.asarray([float(x) for x in buff[1:]],dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-SWW6oqJ5_Zr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "semantics = np.zeros((len(dataset.grapheme), 300), dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C6QCm1rX5_Zu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "for i, w in enumerate(dataset.grapheme):\n",
        "    semantics[i] = semantics_wiki[w]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jkaja0pm5_Zw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pkl_filename = 'SM-nsyl-fastText-win-en-vec.pkl'\n",
        "with open(pkl_filename, 'wb') as f:\n",
        "    pickle.dump(semantics,f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RXPVF7TE5_Zy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ShinAsakawa/xerion/raw/master/data/SM-nsyl-fastText-win-en-vec.pkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eSnv1LYf5_Z1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pkl_filename = 'SM-nsyl-fastText-win-en-vec.pkl'\n",
        "with open(pkl_filename, 'rb') as f:\n",
        "    semantics = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "UNo8NRrY5_Z4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "type(semantics), semantics.shape   # 意味層データの確認"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "czje78VK9_je",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# データの確認のためにデータサイズを印字します\n",
        "dataset.sememe = semantics\n",
        "for modality in [dataset.grapheme, dataset.phoneme, dataset.sememe]:\n",
        "    len(modality), type(modality), modality.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cndCO5Sl5_Z9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset.usage()      # 基本的な使い方を表示させて以下の確認に用います"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o-dHJVd65_aC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 上で表示されたコードに基づいて `sklearn` のニューラルネットワークで予測してみます\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "dataset = Xerion()\n",
        "X = dataset.inputs    \n",
        "y = dataset.outputs\n",
        "# 本例では入力データが grapheme 三連表現，出力データが phoneme 三連表現です\n",
        "model = MLPRegressor()\n",
        "model.fit(X, y)\n",
        "model.score(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-WOjqrz5_aD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 意味層から出力表現への回帰\n",
        "X = semantics\n",
        "y = dataset.outputs\n",
        "model.fit(X,y)\n",
        "model.score(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZKyYoFGV5_aG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 入力データは grapheme 三連表現と fastText wikipedia の連接 105 次元 +300 次元 = 405 次元\n",
        "X = dataset.inputs\n",
        "X = np.concatenate((X, semantics), axis=1)\n",
        "print('X.shape={}'.format(X.shape))\n",
        "model.fit(X, y)\n",
        "model.score(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LjZN_U1v5_aI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 非単語によるモデルの検証"
      ]
    },
    {
      "metadata": {
        "id": "W0-OU1GT5_aI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_ = dataset.make_all()   # 検証データを作成します"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcrNWFPj5_aM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## 作成したデータ名を表示\n",
        "for i, db in enumerate(dataset.dbs.keys()):\n",
        "    print('{0}:{1}'.format(i,db))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "74WGLXuL5_aO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ここでは原著論文と同じく Glushko の非単語リストを用います\n",
        "test_data = dataset.dbs['glushkoNW-nsyl']\n",
        "X_test, y_test = test_data['inputs'], test_data['outputs']\n",
        "X_test.shape, X_test.dtype, y_test.shape, y_test.dtype"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iFPjhwRM5_aS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ちなみに Glushko の非単語リストとは以下の文献になります\n",
        "- year:1979\n",
        "- title: The Organization and Activation of Orthographic Knowledge in Reading Aloud\n",
        "- journal: Journal of Experimental Psychology: Human Perception and Performance, Volume 5, No. 4, pages 674-691.\n",
        "\n",
        "健常者の単語の読みの反応潜時実験を以下に紹介します"
      ]
    },
    {
      "metadata": {
        "id": "lIG7FWXv5_aT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "rt = [529, 546, 550]\n",
        "rt_errors = [0.05, 2.9, 8.3]\n",
        "rt_labels = ['reg/consist','reg/inconsist','except'] #['規則/一貫', '規則/非一貫', '例外語']\n",
        "\n",
        "plt.ylim(500,570)\n",
        "plt.xlabel('word type')\n",
        "plt.ylabel('latency(ms)')\n",
        "plt.title('from Glushko (1979) Tab. 3')\n",
        "plt.bar(x=(0,1,2), height=rt, tick_label=rt_labels)\n",
        "plt.show()\n",
        "# 以下は単語型別の反応潜時です"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nTQA-qhs5_aV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 続いて単語型別のエラー率です\n",
        "plt.ylim(0,10)\n",
        "plt.xlabel('word type')\n",
        "plt.ylabel('error ratio (%)')\n",
        "plt.title('from Glushko (1979) Tab. 3')\n",
        "plt.bar(x=(0,1,2), height=rt_errors, tick_label=rt_labels)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QD_kdyAx5_aW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# データの次元を確認しておきます\n",
        "X_train, y_train = dataset.inputs, dataset.outputs\n",
        "X_test, y_test = dataset.dbs['glushkoNW-nsyl']['inputs'], dataset.dbs['glushkoNW-nsyl']['outputs']\n",
        "for x in [X_train, y_train, X_test, y_test]:\n",
        "    print(x.shape, x.dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "75baTl7t5_aZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)   # もう一度訓練しておきます\n",
        "model.score(X_train, y_train) # 訓練データでの性能を表示\n",
        "model.score(X_test, y_test)   # 非単語リストでの性能の表示"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7VOeqGjn5_ab",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "- Glushko(1979) の非単語リストは非単語だけに意味系の寄与がありません\n",
        "- ですので以上になります"
      ]
    }
  ]
}