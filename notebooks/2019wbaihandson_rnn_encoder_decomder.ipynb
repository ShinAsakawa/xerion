{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019wbaihandson_rnn_encoder-decomder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/xerion/blob/master/notebooks/2019wbaihandson_rnn_encoder_decomder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_0sUaKxEFR70",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# for [第2回失語症ハンズオン](https://wba-initiative.org/3411/)\n",
        "\n",
        "<div>\n",
        "    <center><img src=\"https://wba-initiative.org/wp-content/uploads/2015/05/logo.png\" style=\"width:29%\"></center>\n",
        "</div>\n",
        "    \n",
        "\n",
        "<div align='right'>\n",
        "<a href='mailto:asakawa@ieee.org'>Shin Aasakawa</a>, all rights reserved.<br>\n",
        "Date: 15/Mar/2019<br>\n",
        " MIT license\n",
        "</div>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "hzri8t5tvqFJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# リカレントニューラルネットワーク\n",
        "\n",
        "リカレントニューラルネットワーク (Recurrent neural networks:RNN) は系列データ処理のためのニューラルネットワークモデルです。\n",
        "\n",
        "- 自然言語処理\n",
        "- 時系列予測\n",
        "- 自動翻訳\n",
        "\n",
        "などなど応用範囲が広いです。\n",
        "時刻 $t$ における入力を $x_t$ とすると，活性化関数を $f$, 中間層の出力を $h$, 出力を $y$ とすれば\n",
        "リカレントニューラルネットワークの挙動は以下の式のように表現できます。\n",
        "\n",
        "$$y_{t+1}, h_{t+1} = f(x_{t}, h_{t}),$$\n",
        "\n",
        "以下の図はグレーブスによる通常のリカレントネットワークの表記(左)とその時間展開(右)になります。\n",
        "<!--<img src='images/rnn_basic.png', width=600>-->\n",
        "\n",
        "<center>\n",
        "<!-- <img src='https://drive.google.com/file/d/1M9fIdT_ezJv8k5STiVNd5k0qExQ4U8fd/'  style=\"width:600px\"> -->\n",
        " <img src='http://www.cis.twcu.ac.jp/~asakawa/2019seminar_info/assets/rnn_basic.png' style=\"width:600px\">\n",
        "\n",
        "</center>\n",
        "\n",
        "Image by \n",
        "[Alex Graves](https://www.cs.toronto.edu/~graves/preprint.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "YokbI20NvqFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# エンコーダ=デコーダモデル\n",
        "\n",
        "カルパシィによる 4 つのニューラルネットワークモデルの分類を以下に示しました。\n",
        "* **one-to-one** - 通常の多層パーセプトロン。リカレントニューラルネットワークではありません。画像認識などで使われます\n",
        "* **one-to-many** - 画像から言語を生成する [画像キャプション付けモデル](http://cs.stanford.edu/people/karpathy/deepimagesent/) \n",
        "* **Many-to-one** - 文書の極性判断など\n",
        "* **Many-to-many** 自動翻訳などで使われます。以下のエンコーダ=デコーダモデル参照\n",
        "* **Many-to-many** ロボット制御などで使われます\n",
        "\n",
        "<img src=\"http://www.cis.twcu.ac.jp/~asakawa/2019seminar_info/assets/types.jpeg\" style=\"width:800px\">\n",
        "\n",
        "* [カルパシィのブログ](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "* [オラーのブログ](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) "
      ]
    },
    {
      "metadata": {
        "id": "bGoNsZMZvqFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## エンコーダ=デコーダモデル\n",
        "\n",
        "言語モデルでは，ニューラルネットワークモデルが主流になる前，**長距離依存 long-term dependency** が問題でした。\n",
        "これを解決するモデルが **LSTM (Long Short-Term Memory) 長短期記憶** になります。\n",
        "\n",
        "下の図は簡単な翻訳モデルです。\n",
        "\n",
        "<img src='images/enc-dec.png' style=\"width:400px\">\n",
        "\n",
        "<!--\n",
        "#### Teacher forcing\n",
        "We will also use what is called *teacher forcing*.\n",
        "This is shown as the gray lines in the figure.\n",
        "This means that the RNN will implement a sequence of conditional distributions so the $t$th output of the decoder gives $p(y_t|y_1,\\dots,y_{t-1} ,x)$. This formulation will make the task easier and faster for the network to learn because it during training always have access to the correct preceding outputs.\n",
        "A test time where we we don't know the output sequence we have to predict one step at a time. \n",
        "There is no guarantee that we will find the mostly likely decoded *sequence*. A technique called [beam search](https://arxiv.org/pdf/1702.01806.pdf) is used in machine translation and related tasks to look for list of candidate decoded sequences.    \n",
        "\n",
        "\n",
        "\n",
        "#### Alternatives\n",
        "There are other ways to let the encoder and decoder communicate with each other.\n",
        "For instance you can give the last state of the Encoder as input to the Decoder at each decode time step, not just the previously predicted word.\n",
        "Another approache is called **attention**, which lets the Decoder attend to different parts of the encoded input at different timesteps in the decoding process. \n",
        "Attention is shown in the next notebook. \n",
        "-->\n"
      ]
    },
    {
      "metadata": {
        "id": "AEectdQwvqFM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 実習\n",
        "\n",
        "以下は Keras のサンプルコード\n",
        "[addtion_rnn.py](https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py) を修正したものです。\n",
        "3 桁までの足し算を実行するニューラルネットワークになります。\n",
        "1980年代は **選択的忘却** あるいは **カタストロフィック忘却** と言って，覚えたルールをすぐに忘れるということが問題になっていました。\n",
        "それを解決するのがリカレントニューラルネットワークモデルで，比較的簡単な実装で体感できます。\n",
        "\n",
        "一般にリカレントニューラルネットワークは学習に時間を要し，かつ，**黒魔術** と揶揄されてきた特別な技法が必要でした。\n",
        "これは勾配消失問題，勾配爆発問題と密接に関連します。\n",
        "勾配チェック，勾配クリップ，勾配正規化，リカレントニューラル層の対角初期化など，本セミナーでは触れません。\n"
      ]
    },
    {
      "metadata": {
        "id": "eSPdk0-IvqFN",
        "colab_type": "code",
        "outputId": "346230ed-95d2-432f-ea4f-87c392cba37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "'''\n",
        "# An implementation of sequence to sequence learning for performing addition\n",
        "\n",
        "Input: \"535+61\"\n",
        "Output: \"596\"\n",
        "Padding is handled by using a repeated sentinel character (space)\n",
        "\n",
        "Input may optionally be reversed, shown to increase performance in many tasks in:\n",
        "\"Learning to Execute\"\n",
        "http://arxiv.org/abs/1410.4615\n",
        "and\n",
        "\"Sequence to Sequence Learning with Neural Networks\"\n",
        "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
        "Theoretically it introduces shorter term dependencies between source and target.\n",
        "\n",
        "Two digits reversed:\n",
        "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
        "\n",
        "Three digits reversed:\n",
        "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
        "\n",
        "Four digits reversed:\n",
        "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
        "\n",
        "Five digits reversed:\n",
        "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
        "'''  \n",
        "# noqa\n",
        "\n",
        "# 必要なライブラリを輸入 import します \n",
        "from __future__ import print_function\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "from six.moves import range"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XW5dll2gvqFU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharacterTable(object):\n",
        "    \"\"\"Given a set of characters:\n",
        "    ワンホットエンコーディングをするクラスの定義\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Oe2Pny5vqFX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class colors:\n",
        "    ok = '\\033[92m'\n",
        "    fail = '\\033[91m'\n",
        "    close = '\\033[0m'\n",
        "\n",
        "# パラメータとデータセットの定義\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = True\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = '0123456789+ '\n",
        "ctable = CharacterTable(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q-wMLSTCvqFZ",
        "colab_type": "code",
        "outputId": "23c3ea54-a527-4adc-afe5-c2959d9f3552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print('データの生成...')\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
        "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = '{}+{}'.format(a, b)\n",
        "    query = q + ' ' * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print('全問題数:', len(questions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "データの生成...\n",
            "全問題数: 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wiv747_NvqFd",
        "colab_type": "code",
        "outputId": "acdca509-e85a-4b11-98ad-2c3d1f9dae6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "print('入出力データの準備...')\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "# digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print('訓練データ:')\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print('検証データ:')\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "\n",
        "# LSTM を用いていますが，単純RNN (SimplerNN) や GRU に入れ替えることもできます\n",
        "RNN = layers.GRU\n",
        "HIDDEN_SIZE = 128  # 各層内のニューロン数\n",
        "BATCH_SIZE = 128   # ミニバッチサイズ\n",
        "LAYERS = 2         # 中間層数"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "入出力データの準備...\n",
            "訓練データ:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "検証データ:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kGaOIluXvqFh",
        "colab_type": "code",
        "outputId": "6a5f4a69-0a1a-4dfe-ae4f-e4fa64276be5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "cell_type": "code",
      "source": [
        "print('モデル作成...')\n",
        "model = Sequential()\n",
        "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
        "# Note: In a situation where your input sequences have a variable length,\n",
        "# use input_shape=(None, num_feature).\n",
        "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
        "# As the decoder RNN's input, repeatedly provide with the last output of\n",
        "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "# The decoder RNN could be multiple layers stacked or a single layer.\n",
        "for _ in range(LAYERS):\n",
        "    # By setting return_sequences to True, return not only the last output but\n",
        "    # all the outputs so far in the form of (num_samples, timesteps,\n",
        "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
        "    # the first dimension to be the timesteps.\n",
        "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
        "\n",
        "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
        "# of the output sequence, decide which character should be chosen.\n",
        "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "モデル作成...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 128)               72192     \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 4, 12)             1548      \n",
            "=================================================================\n",
            "Total params: 336,908\n",
            "Trainable params: 336,908\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C8PbuDebvqFk",
        "colab_type": "code",
        "outputId": "c1e33d6a-d4b6-4f66-9243-3a5e413d1eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2404
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for iteration in range(1, 200):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration)\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=1,\n",
        "              validation_data=(x_val, y_val))\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = model.predict_classes(rowx, verbose=0)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
        "        print('T', correct, end=' ')\n",
        "        if correct == guess:\n",
        "            print(colors.ok + '☑' + colors.close, end=' ')\n",
        "        else:\n",
        "            print(colors.fail + '☒' + colors.close, end=' ')\n",
        "        print(guess)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Iteration 1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 29s 641us/step - loss: 1.9099 - acc: 0.3203 - val_loss: 1.8149 - val_acc: 0.3397\n",
            "Q 393+73  T 466  \u001b[91m☒\u001b[0m 104 \n",
            "Q 890+76  T 966  \u001b[91m☒\u001b[0m 104 \n",
            "Q 963+3   T 966  \u001b[91m☒\u001b[0m 104 \n",
            "Q 858+23  T 881  \u001b[91m☒\u001b[0m 104 \n",
            "Q 75+7    T 82   \u001b[91m☒\u001b[0m 14  \n",
            "Q 712+306 T 1018 \u001b[91m☒\u001b[0m 104 \n",
            "Q 277+891 T 1168 \u001b[91m☒\u001b[0m 104 \n",
            "Q 260+73  T 333  \u001b[91m☒\u001b[0m 104 \n",
            "Q 30+122  T 152  \u001b[91m☒\u001b[0m 144 \n",
            "Q 850+7   T 857  \u001b[91m☒\u001b[0m 144 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 2\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 26s 586us/step - loss: 1.7492 - acc: 0.3453 - val_loss: 1.6739 - val_acc: 0.3673\n",
            "Q 85+294  T 379  \u001b[91m☒\u001b[0m 748 \n",
            "Q 10+31   T 41   \u001b[91m☒\u001b[0m 11  \n",
            "Q 684+887 T 1571 \u001b[91m☒\u001b[0m 1566\n",
            "Q 830+89  T 919  \u001b[91m☒\u001b[0m 904 \n",
            "Q 823+429 T 1252 \u001b[91m☒\u001b[0m 1236\n",
            "Q 1+14    T 15   \u001b[91m☒\u001b[0m 11  \n",
            "Q 350+972 T 1322 \u001b[91m☒\u001b[0m 1568\n",
            "Q 839+46  T 885  \u001b[91m☒\u001b[0m 904 \n",
            "Q 97+866  T 963  \u001b[91m☒\u001b[0m 1016\n",
            "Q 510+25  T 535  \u001b[91m☒\u001b[0m 744 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 3\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 26s 587us/step - loss: 1.5873 - acc: 0.4021 - val_loss: 1.5140 - val_acc: 0.4318\n",
            "Q 879+557 T 1436 \u001b[91m☒\u001b[0m 1332\n",
            "Q 98+0    T 98   \u001b[91m☒\u001b[0m 11  \n",
            "Q 68+316  T 384  \u001b[91m☒\u001b[0m 282 \n",
            "Q 71+35   T 106  \u001b[91m☒\u001b[0m 11  \n",
            "Q 583+627 T 1210 \u001b[91m☒\u001b[0m 1112\n",
            "Q 5+643   T 648  \u001b[91m☒\u001b[0m 562 \n",
            "Q 564+787 T 1351 \u001b[91m☒\u001b[0m 1232\n",
            "Q 10+11   T 21   \u001b[91m☒\u001b[0m 11  \n",
            "Q 50+615  T 665  \u001b[91m☒\u001b[0m 562 \n",
            "Q 998+746 T 1744 \u001b[91m☒\u001b[0m 1556\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 4\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 27s 590us/step - loss: 1.4312 - acc: 0.4619 - val_loss: 1.3606 - val_acc: 0.4898\n",
            "Q 22+391  T 413  \u001b[91m☒\u001b[0m 349 \n",
            "Q 30+261  T 291  \u001b[91m☒\u001b[0m 279 \n",
            "Q 504+48  T 552  \u001b[91m☒\u001b[0m 549 \n",
            "Q 69+19   T 88   \u001b[91m☒\u001b[0m 10  \n",
            "Q 846+827 T 1673 \u001b[91m☒\u001b[0m 1706\n",
            "Q 539+877 T 1416 \u001b[91m☒\u001b[0m 1495\n",
            "Q 375+50  T 425  \u001b[91m☒\u001b[0m 449 \n",
            "Q 255+356 T 611  \u001b[91m☒\u001b[0m 648 \n",
            "Q 693+64  T 757  \u001b[91m☒\u001b[0m 748 \n",
            "Q 194+9   T 203  \u001b[91m☒\u001b[0m 100 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 5\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 27s 590us/step - loss: 1.3091 - acc: 0.5086 - val_loss: 1.2593 - val_acc: 0.5253\n",
            "Q 34+700  T 734  \u001b[91m☒\u001b[0m 780 \n",
            "Q 456+18  T 474  \u001b[91m☒\u001b[0m 590 \n",
            "Q 229+85  T 314  \u001b[91m☒\u001b[0m 399 \n",
            "Q 899+5   T 904  \u001b[91m☒\u001b[0m 990 \n",
            "Q 591+564 T 1155 \u001b[91m☒\u001b[0m 1133\n",
            "Q 314+268 T 582  \u001b[91m☒\u001b[0m 676 \n",
            "Q 762+47  T 809  \u001b[91m☒\u001b[0m 870 \n",
            "Q 86+282  T 368  \u001b[91m☒\u001b[0m 433 \n",
            "Q 13+429  T 442  \u001b[91m☒\u001b[0m 393 \n",
            "Q 30+512  T 542  \u001b[91m☒\u001b[0m 571 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 6\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 26s 588us/step - loss: 1.2180 - acc: 0.5442 - val_loss: 1.1573 - val_acc: 0.5726\n",
            "Q 558+746 T 1304 \u001b[91m☒\u001b[0m 1335\n",
            "Q 313+938 T 1251 \u001b[91m☒\u001b[0m 1237\n",
            "Q 148+90  T 238  \u001b[91m☒\u001b[0m 224 \n",
            "Q 78+42   T 120  \u001b[91m☒\u001b[0m 101 \n",
            "Q 136+97  T 233  \u001b[91m☒\u001b[0m 224 \n",
            "Q 341+807 T 1148 \u001b[91m☒\u001b[0m 1111\n",
            "Q 512+845 T 1357 \u001b[91m☒\u001b[0m 1394\n",
            "Q 81+856  T 937  \u001b[91m☒\u001b[0m 949 \n",
            "Q 97+933  T 1030 \u001b[91m☒\u001b[0m 1031\n",
            "Q 19+323  T 342  \u001b[91m☒\u001b[0m 334 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 7\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 27s 591us/step - loss: 1.1320 - acc: 0.5749 - val_loss: 1.0857 - val_acc: 0.5898\n",
            "Q 500+378 T 878  \u001b[91m☒\u001b[0m 818 \n",
            "Q 599+10  T 609  \u001b[91m☒\u001b[0m 611 \n",
            "Q 91+740  T 831  \u001b[91m☒\u001b[0m 813 \n",
            "Q 512+845 T 1357 \u001b[91m☒\u001b[0m 1318\n",
            "Q 501+9   T 510  \u001b[91m☒\u001b[0m 518 \n",
            "Q 764+109 T 873  \u001b[91m☒\u001b[0m 818 \n",
            "Q 13+66   T 79   \u001b[91m☒\u001b[0m 80  \n",
            "Q 731+102 T 833  \u001b[91m☒\u001b[0m 838 \n",
            "Q 982+768 T 1750 \u001b[91m☒\u001b[0m 1718\n",
            "Q 98+751  T 849  \u001b[91m☒\u001b[0m 813 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 8\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "45000/45000 [==============================] - 26s 586us/step - loss: 1.0486 - acc: 0.6058 - val_loss: 1.0044 - val_acc: 0.6116\n",
            "Q 702+434 T 1136 \u001b[91m☒\u001b[0m 1188\n",
            "Q 858+23  T 881  \u001b[91m☒\u001b[0m 886 \n",
            "Q 431+256 T 687  \u001b[91m☒\u001b[0m 676 \n",
            "Q 91+740  T 831  \u001b[91m☒\u001b[0m 806 \n",
            "Q 98+49   T 147  \u001b[91m☒\u001b[0m 148 \n",
            "Q 1+314   T 315  \u001b[91m☒\u001b[0m 321 \n",
            "Q 822+76  T 898  \u001b[91m☒\u001b[0m 896 \n",
            "Q 522+65  T 587  \u001b[91m☒\u001b[0m 586 \n",
            "Q 89+818  T 907  \u001b[91m☒\u001b[0m 809 \n",
            "Q 18+33   T 51   \u001b[92m☑\u001b[0m 51  \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 9\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "39296/45000 [=========================>....] - ETA: 3s - loss: 0.9539 - acc: 0.6392"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tLRqfWtLvqFp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}