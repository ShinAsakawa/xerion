{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019wbaihandson_tiny-char-rnn-ja.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/xerion/blob/master/notebooks/2019wbaihandson_tiny_char_rnn_ja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_ejP2yGrDYZ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# [第2回失語症ハンズオン](https://wba-initiative.org/3411/)\n",
        "\n",
        "<div>\n",
        "    <center><img src=\"https://wba-initiative.org/wp-content/uploads/2015/05/logo.png\" style=\"width:29%\"></center>\n",
        "</div>\n",
        "    \n",
        "\n",
        "<div align='right'>\n",
        "<a href='mailto:asakawa@ieee.org'>Shin Aasakawa</a>, all rights reserved.<br>\n",
        "Date: 15/Mar/2019<br>\n",
        " MIT license\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "# tiny 文字RNN による文字予測\n",
        "\n",
        "[Original](https://github.com/ekzhang/char-rnn-keras) <https://github.com/ekzhang/char-rnn-keras>"
      ]
    },
    {
      "metadata": {
        "id": "YS0ppYQjDpQJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- <font size=\"+2\" color='blue'>文字ベースの RNN を使って簡単な予測を行ってみます</font>"
      ]
    },
    {
      "metadata": {
        "id": "EUwkAnNMDYZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 必要なライブラリの輸入 `import`\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pSey5K_PDYaD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# モデルの作成。n_cells と n_layers とを変更して性能を確認してください\n",
        "def build_model(batch_size, seq_len, vocab_size, n_cells=128, n_layers=2):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, n_cells, batch_input_shape=(batch_size, seq_len)))\n",
        "    for i in range(n_layers):\n",
        "        model.add(LSTM(n_cells, return_sequences=True, stateful=True))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(TimeDistributed(Dense(vocab_size)))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "noQ5Qd7mDYaG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ここでは簡単のため，ごく短い文章を学習させてみます。日本国憲法第9条\n",
        "text = \"\".join('第９条 日本国民は、正義と秩序を基調とする国際平和を誠実に希求し、\\n')\n",
        "text += \"\".join('国権の発動たる戦争と、武力による威嚇又は武力の行使は、国際紛争を解決する手段としては、永久にこれを放棄する。\\n')\n",
        "text += \"\".join('２項 前項の目的を達するため、陸海空軍その他の戦力は、これを保持しない。国の交戦権は、これを認めない。\\n')\n",
        "\n",
        "chr2idx = {ch: i for (i, ch) in enumerate(sorted(list(set(text))))}\n",
        "idx2chr = {idx:char for char, idx in chr2idx.items()}\n",
        "vocab_size = len(chr2idx)\n",
        "\n",
        "Data = np.asarray([chr2idx[c] for c in text], dtype=np.int32)\n",
        "for d in Data:\n",
        "    print(idx2chr[d], end='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y6xPH_13DYaJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 14  # ミニバッチのサイズ\n",
        "SEQ_LENGTH = 8   # 一つの系列の長さ\n",
        "\n",
        "def make_batches(D, vocab_size, batch_size=BATCH_SIZE, seq_length=SEQ_LENGTH):\n",
        "    #length = X.shape[0]\n",
        "    n_chars = D.shape[0] // batch_size\n",
        "\n",
        "    for start in range(0, n_chars - seq_length, seq_length):\n",
        "        X = np.zeros((batch_size, seq_length))\n",
        "        Y = np.zeros((batch_size, seq_length, vocab_size))\n",
        "        for idx in range(0, batch_size):\n",
        "            for i in range(0, seq_length):\n",
        "                X[idx, i] = D[n_chars * idx + start + i]\n",
        "                Y[idx, i, D[n_chars * idx + start + i + 1]] = 1\n",
        "        yield X, Y\n",
        "        \n",
        "for (X, Y) in make_batches(Data, vocab_size, batch_size=BATCH_SIZE, seq_length=SEQ_LENGTH):\n",
        "    print(X.shape, Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vh-K0ClTDYaM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# モデルを作成してコンパイルします\n",
        "model = build_model(BATCH_SIZE, SEQ_LENGTH, vocab_size)\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "WiN27rixDYaO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 20    # エポック数だけ学習します。20 回では精度が十分ではありません。何度か繰り返します\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
        "    losses, accs = [], []\n",
        "    for i, (X, Y) in enumerate(make_batches(Data, vocab_size, batch_size=BATCH_SIZE, seq_length=SEQ_LENGTH)):\n",
        "    #for i, (X, Y) in enumerate(make_batches(Data, vocab_size)):\n",
        "        loss, acc = model.train_on_batch(X, Y)\n",
        "        print('Batch {}: loss = {:.4f}, acc = {:.5f}'.format(i + 1, loss, acc))\n",
        "        losses.append(loss)\n",
        "        accs.append(acc)\n",
        "        \n",
        "    yhat = model.predict(X)\n",
        "    for y in yhat:\n",
        "        for c in y:\n",
        "            print(idx2chr[np.argmax(c)], end='')\n",
        "        print('/', end='')\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ipiReM8wDYaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 実際に予測できたか試してみましょう\n",
        "a = model.predict(X)\n",
        "for x in a:\n",
        "    for l in x:\n",
        "        print(idx2chr[np.argmax(l)], end='')\n",
        "    print('/',end='')\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mS9W2owZDYaT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 結果を保存します。保存する必要もないですが，一応\n",
        "model.save_weights('j_const9_weights.h5')\n",
        "model.save('j_const9_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}