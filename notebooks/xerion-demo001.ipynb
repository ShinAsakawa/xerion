{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer of Xerion, data management for simulating SM89 and PMSP96\n",
    "\n",
    "for [wbai handson](https://wba-initiative.org/3411/)\n",
    "\n",
    "<div>\n",
    "    <center><img src=\"https://wba-initiative.org/wp-content/uploads/2015/05/logo.png\" style=\"width:29%\"></center>\n",
    "</div>\n",
    "    \n",
    "\n",
    "<p style='text-align:center;'>\n",
    "    <font color='green' size='+1' style='font-weight:bold;'>Shin Asakawa &lt;asakawa@ieee.org&gt;</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 手順1. `Git clone` する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub から必要なパッケージを入手してください\n",
    "#!git clone https://github.com/ShinAsakawa/wbai_aphasia\n",
    "#\n",
    "# 上で入手したパッケージをインストールします\n",
    "#!cd xerion; pip install .\n",
    "#\n",
    "# インストール結果を確認します\n",
    "#import xerion as handson\n",
    "#data = handson.Xerion()\n",
    "##descr() はデータの諸元を表示します\n",
    "#data.descr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 手順2. データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xerion.xerion import Xerion\n",
    "handson = Xerion()\n",
    "\n",
    "# Xerion は data 管理用モジュールです。以下のコメントを外してご覧ください\n",
    "#handson.descr()\n",
    "#handson.usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 2995, 2996, 2997], dtype=int16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grapheme = handson.db['grapheme']\n",
    "phoneme = handson.db['phoneme']\n",
    "seq = handson.db['seq']\n",
    "len(handson.db['grapheme'])\n",
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 手順3. 読み込んだデータの確認\n",
    "\n",
    "ここに `input`, `output`, `freq`, `grapheme`, `phoneme`, `seq`, `tag` でアクセスできるようにしてあります。\n",
    "\n",
    "- input: np.ndarrray((2998, 105), dtype=float32) # 文字単語のトリプレット表現 105 次元ベクトル\n",
    "- output: np.ndarray((2998, 61), dtype=float32)  # 対応する音韻トリプレット表現 61 次元ベクトル\n",
    "- frep: np.ndarray((2998,), dtype=float32)       # 対応する頻度情報\n",
    "- seq: \n",
    "- grapheme: 入力文字列リスト\n",
    "- phoneme: 出力音韻列リスト ARPABET 表現\n",
    "- tag: 無視してください\n",
    "\n",
    "SM89, PMSP96 では意味層は実装されていません。<br>\n",
    "以下に例を示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#', 'seq', 'grapheme', 'phoneme', 'freq', 'tag', 'inputs', 'outputs')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['ace', 'ache', 'act', ..., 'zone', 'zoo', 'zounds'], dtype='<U8'),\n",
       " array(['As', 'Ak', '@kt', ..., 'zOn', 'zU', 'zWnz'], dtype='<U6'),\n",
       " array([0.236101, 0.149313, 0.471041, ..., 0.213746, 0.199825, 0.115525],\n",
       "       dtype=float32),\n",
       " array([   0,    1,    2, ..., 2995, 2996, 2997], dtype=int16),\n",
       " array([[0, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16),\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16),\n",
       " {'#',\n",
       "  'AMB',\n",
       "  'CON',\n",
       "  'EXC',\n",
       "  'EXPT',\n",
       "  'HEC',\n",
       "  'HFE',\n",
       "  'HFEEXPT',\n",
       "  'HFRIC',\n",
       "  'HRI',\n",
       "  'HSTR',\n",
       "  'HSTRUNQ',\n",
       "  'LEC',\n",
       "  'LFE',\n",
       "  'LFEEXPT',\n",
       "  'LFRI',\n",
       "  'LRIC',\n",
       "  'LRICCON',\n",
       "  'LSTR',\n",
       "  'LSTRUNQ',\n",
       "  'REG',\n",
       "  'RI',\n",
       "  'STR',\n",
       "  'STREXPT',\n",
       "  'UNQ',\n",
       "  'homg'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(handson.tags)\n",
    "handson.grapheme, handson.phoneme, handson.freq, handson.seq, handson.inputs, handson.outputs, set(handson.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##最初，このセルは無視してください\n",
    "#print(handson.url_base, handson.url_file)\n",
    "#print(handson.basefilename)\n",
    "#print(handson.syl_files, '\\n', handson.nsyl_files)\n",
    "#print(handson.datadir)\n",
    "#print(handson.datafilenames)\n",
    "#print(type(handson.db), len(handson.db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs (2998, 105)\n",
      "outputs (2998, 61)\n",
      "grapheme (2998,)\n",
      "phoneme (2998,)\n",
      "freq (2998,)\n",
      "tag 2998\n",
      "seq (2998,)\n"
     ]
    }
   ],
   "source": [
    "type(handson.db)\n",
    "for k in handson.db.keys():\n",
    "    print(k, end=' ')\n",
    "    if isinstance(handson.db[k], list):\n",
    "        print(len(handson.db[k]))\n",
    "    elif isinstance(handson.db[k], np.ndarray):\n",
    "        print(handson.db[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.236101, 0.149313, 0.471041, ..., 0.213746, 0.199825, 0.115525],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['ace', 'ache', 'act', ..., 'zone', 'zoo', 'zounds'], dtype='<U8')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['As', 'Ak', '@kt', ..., 'zOn', 'zU', 'zWnz'], dtype='<U6')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2998, (2998, 105), array([[0, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2998, (2998, 61), array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 各データを表示します\n",
    "handson.freq        ### データ内の各単語の頻度\n",
    "handson.grapheme    ### 各単語の表記表現\n",
    "handson.phoneme     ### 各単語の音韻表現\n",
    "len(handson.inputs), handson.inputs.shape, handson.inputs     ###入力表現 numpy の ndarray\n",
    "len(handson.outputs), handson.outputs.shape, handson.outputs  ###出力表現 numpy の ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'S', 'P', 'T', 'K', 'Q', 'C', 'B', 'D', 'G', 'F', 'V', 'J', 'Z', 'L', 'M', 'N', 'R', 'W', 'H', 'CH', 'GH', 'GN', 'PH', 'PS', 'RH', 'SH', 'TH', 'TS', 'WH']\n",
      "['E', 'I', 'O', 'U', 'A', 'Y', 'AI', 'AU', 'AW', 'AY', 'EA', 'EE', 'EI', 'EU', 'EW', 'EY', 'IE', 'OA', 'OE', 'OI', 'OO', 'OU', 'OW', 'OY', 'UE', 'UI', 'UY']\n",
      "['H', 'R', 'L', 'M', 'N', 'B', 'D', 'G', 'C', 'X', 'F', 'V', '∫', 'S', 'Z', 'P', 'T', 'K', 'Q', 'BB', 'CH', 'CK', 'DD', 'DG', 'FF', 'GG', 'GH', 'GN', 'KS', 'LL', 'NG', 'NN', 'PH', 'PP', 'PS', 'RR', 'SH', 'SL', 'SS', 'TCH', 'TH', 'TS', 'TT', 'ZZ', 'U', 'E', 'ES', 'ED']\n",
      "['s', 'S', 'C', 'z', 'Z', 'j', 'f', 'v', 'T', 'D', 'p', 'b', 't', 'd', 'k', 'g', 'm', 'n', 'h', 'I', 'r', 'w', 'y']\n",
      "['a', 'e', 'i', 'o', 'u', '@', '^', 'A', 'E', 'I', 'O', 'U', 'W', 'Y']\n",
      "['r', 'I', 'm', 'n', 'N', 'b', 'g', 'd', 'ps', 'ks', 'ts', 's', 'z', 'f', 'v', 'p', 'k', 't', 'S', 'Z', 'T', 'D', 'C', 'j']\n"
     ]
    }
   ],
   "source": [
    "#以下に 3 連表現を示します。PMSP96論文中では triplet と呼ばれていたオンセット onset，母音 vowel, コーダ coda です。\n",
    "for item in [handson.Orthography, handson.Phonology]:\n",
    "    for entry in ['onset', 'vowel', 'coda']:\n",
    "        print(item[entry])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 同形異音語について"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2998, numpy.ndarray)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2985"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ総数:2998 とユニークなデータ数:2985 との差が同形異音語数になります\n"
     ]
    }
   ],
   "source": [
    "len(handson.grapheme), type(handson.grapheme)\n",
    "grapheme_set = set(handson.grapheme)\n",
    "len(grapheme_set)\n",
    "\n",
    "print('データ総数:{0} とユニークなデータ数:{1} との差が同形異音語数になります'.format(\n",
    "    len(handson.grapheme),len(grapheme_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bass bow dove house lead live read route row sow tear wind wound \n",
      "n_homographs=13\n"
     ]
    }
   ],
   "source": [
    "# 実際に同形異音語を表示してみましょう\n",
    "prev_word = \"\"\n",
    "n_homographs = 0\n",
    "for word in sorted(handson.grapheme):\n",
    "    if word == prev_word:\n",
    "        n_homographs += 1\n",
    "        print(word, end=' ')\n",
    "    prev_word = word\n",
    "print('\\nn_homographs={}'.format(n_homographs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 以下は知らなくても良い情報ですので無視してください for debuging\n",
    "#handson.load_pickle()\n",
    "#data.make_all()\n",
    "#print(handson.module_path)\n",
    "#handson.note()\n",
    "#handson.nsyl_files\n",
    "#handson.origfile_size\n",
    "#print(handson.Orthography)\n",
    "#print(handson.Phonology)\n",
    "#print(handson.pkl_dir)\n",
    "#handson.url_base\n",
    "#handson.url_file\n",
    "#handson.xerion_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database:  SM-nsyl\n"
     ]
    }
   ],
   "source": [
    "for db in handson.dbs:\n",
    "    print('database: ', db) # , db.dbs[db][3].shape, type(db.dbs[db][3])) #, x.dbs[db][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# データないに登録された各単語の発音を調べる\n",
    "\n",
    "- pyhon で自然言語処理する際に使われる `nltk` を使って単語の発音を調べてみます。\n",
    "- `nltk` については [https://www.nltk.org/](https://www.nltk.org/) をご覧ください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /Users/asakawa/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colaboratory では cmudict がダウンロードされいないようなのでダウンロードします\n",
    "# 一度だけ実行すればよいので，その都度実行する必要はありません\n",
    "import nltk\n",
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/2998=1.83 percent words in Xerion data were not registered in ARPABET.\n",
      "['awn', 'bas', 'bilge', 'bleat', 'calve', 'cheep', 'chive', 'clod', 'clop', 'clove', 'crag', 'cud', 'dork', 'fiche', 'flays', 'frappe', 'git', 'gunk', 'hasp', 'jag', 'jape', 'laze', 'leer', 'letch', 'manse', 'muss', 'nape', 'paunch', 'phage', 'pram', 'rend', 'retch', 'rheum', 'rime', 'runt', 'scrim', 'shoal', 'shush', 'snoot', 'sooth', 'souse', 'sprig', 'squaw', 'thwack', 'tog', 'tyme', 'ump', 'veldt', 'vend', 'whelp', 'whil', 'whir', 'wretch', 'yawl', 'zit']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "arpabet = cmudict.dict()\n",
    "\n",
    "# 1. Xerion の単語を取り出します\n",
    "words = handson.db['grapheme']\n",
    "\n",
    "# 2. Xerion に存在する単語のうち cmudict に発音の登録のない単語を抜き出します。\n",
    "not_in_arpabet = [word for word in words if not word in arpabet]\n",
    "\n",
    "print('{0}/{1}={2:.2f} percent words in Xerion data were not registered in ARPABET.'\n",
    "      .format(len(not_in_arpabet), len(words), len(not_in_arpabet)/len(words)*100))\n",
    "\n",
    "# 随分ありますね。\n",
    "print(not_in_arpabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123455\n",
      "['a', 'a.', 'a42128', 'aaa', 'aaberg']\n",
      "[[['AH0'], ['EY1']], [['EY1']], [['EY1', 'F', 'AO1', 'R', 'T', 'UW1', 'W', 'AH1', 'N', 'T', 'UW1', 'EY1', 'T']], [['T', 'R', 'IH2', 'P', 'AH0', 'L', 'EY1']], [['AA1', 'B', 'ER0', 'G']]]\n"
     ]
    }
   ],
   "source": [
    "# ちなみに cmu 辞書についてですが\n",
    "print(len(arpabet))\n",
    "\n",
    "arpabet_vocab = [v for v in arpabet.keys()]\n",
    "print(arpabet_vocab[:5])\n",
    "arpabet_sounds = [s for s in arpabet.values()]\n",
    "print(arpabet_sounds[:5])\n",
    "#上記の表記については ARPABET をご覧ください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ARPABET に登録されていない単語の処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awn [['AO1', 'EH1', 'N']]\n",
      "bas [['B', 'IY1', 'AE1', 'Z'], ['B', 'IY1', 'EH1', 'Z']]\n",
      "bilge [['B', 'IH1', 'L', 'JH', 'IY1', 'IY1']]\n",
      "bleat [['B', 'L', 'IY1', 'T', 'IY1']]\n",
      "calve [['K', 'AE1', 'L', 'V', 'IY1'], ['K', 'AE1', 'L', 'V', 'IY1', 'IY1']]\n",
      "cheep [['CH', 'EY1', 'IY1', 'P', 'IY1']]\n",
      "chive [['K', 'AY1', 'V', 'IY1'], ['K', 'AY1', 'V', 'IY1', 'IY1']]\n",
      "clod [['S', 'IY1', 'L', 'OW1', 'D', 'IY1']]\n",
      "clop [['S', 'IY1', 'L', 'AA1', 'P']]\n",
      "clove [['S', 'IY1', 'L', 'AH1', 'V']]\n",
      "crag [['S', 'IY1', 'R', 'AE1', 'G']]\n",
      "cud [['S', 'IY1', 'Y', 'UW1', 'D', 'IY1']]\n",
      "dork [['D', 'UW1', 'AA1', 'R', 'K', 'EY1']]\n",
      "fiche [['F', 'AY1', 'CH', 'EY1'], ['F', 'IY1', 'CH', 'EY1']]\n",
      "flays [['F', 'L', 'EY1', 'EH1', 'S']]\n",
      "frappe [['F', 'R', 'AE1', 'P', 'P', 'IY1', 'IY1']]\n",
      "git [['JH', 'IY1', 'IH1', 'T'], ['JH', 'IY1', 'IH0', 'T']]\n",
      "gunk [['G', 'UW1', 'EH1', 'N', 'K', 'EY1']]\n",
      "hasp [['HH', 'AA1', 'EH1', 'S', 'P', 'IY1'], ['EY1', 'CH', 'EY1', 'EH1', 'S', 'P', 'IY1']]\n",
      "jag [['Y', 'AA1', 'JH', 'IY1']]\n",
      "jape [['Y', 'AA1', 'P', 'IY1', 'IY1']]\n",
      "laze [['L', 'AA1', 'Z', 'IY1']]\n",
      "leer [['L', 'AH0', 'ER0']]\n",
      "letch [['L', 'EH1', 'T', 'S', 'IY1', 'EY1', 'CH']]\n",
      "manse [['M', 'AE1', 'N', 'S', 'AW2', 'TH', 'IY1', 'S', 'T'], ['M', 'AE1', 'N', 'S', 'EY1'], ['M', 'AE1', 'N', 'EH1', 'S', 'IY1']]\n",
      "muss [['M', 'UW1', 'EH1', 'S', 'EH1', 'S']]\n",
      "nape [['N', 'AA1', 'P', 'IY1', 'IY1']]\n",
      "paunch [['P', 'AA1', 'AH1', 'N', 'S', 'IY1', 'EY1', 'CH'], ['P', 'AA1', 'Y', 'UW1', 'EH1', 'N', 'S', 'IY1', 'EY1', 'CH']]\n",
      "phage [['P', 'IY1', 'EY1', 'CH', 'EY1', 'JH']]\n",
      "pram [['P', 'IY1', 'R', 'AE1', 'M']]\n",
      "rend [['R', 'EY1', 'EH1', 'N', 'D', 'IY1'], ['R', 'IY1', 'EH1', 'N', 'D', 'IY1']]\n",
      "retch [['R', 'EH1', 'T', 'S', 'IY1', 'EY1', 'CH']]\n",
      "rheum [['AA1', 'R', 'HH', 'IY1', 'AH1', 'M']]\n",
      "rime [['R', 'IH1', 'M', 'IY1']]\n",
      "runt [['R', 'UW1', 'EH1', 'N', 'T', 'IY1'], ['AA1', 'R', 'Y', 'UW1', 'EH1', 'N', 'T', 'IY1']]\n",
      "scrim [['EH1', 'S', 'K', 'R', 'IH1', 'M']]\n",
      "shoal [['EH1', 'S', 'EY1', 'CH', 'OW1', 'AE1', 'L']]\n",
      "shush [['SH', 'UW1', 'EH1', 'S', 'EY1', 'CH']]\n",
      "snoot [['EH1', 'S', 'N', 'OW1', 'AO1', 'T'], ['EH1', 'S', 'N', 'OW1', 'OW1', 'T', 'IY1']]\n",
      "sooth [['S', 'UW1', 'T', 'IY1', 'EY1', 'CH']]\n",
      "souse [['S', 'OW1', 'Y', 'UW1', 'S'], ['S', 'OW1', 'Y', 'UW1', 'Z']]\n",
      "sprig [['EH1', 'S', 'P', 'R', 'IH1', 'G']]\n",
      "squaw [['EH1', 'S', 'K', 'UW1', 'AO1']]\n",
      "thwack [['T', 'IY1', 'EY1', 'CH', 'W', 'AE1', 'K']]\n",
      "tog [['T', 'UW1', 'JH', 'IY1'], ['T', 'IH0', 'JH', 'IY1'], ['T', 'AH0', 'JH', 'IY1']]\n",
      "tyme [['T', 'AY1', 'M', 'IY1']]\n",
      "ump [['AH1', 'M', 'P', 'IY1']]\n",
      "veldt [['V', 'IY1', 'EH1', 'L', 'D', 'IY1', 'T', 'IY1'], ['V', 'IY1', 'IY1', 'EH1', 'L', 'D', 'IY1', 'T', 'IY1']]\n",
      "vend [['V', 'IY1', 'EH1', 'N', 'D', 'IY1'], ['V', 'IY1', 'IY1', 'EH1', 'N', 'D', 'IY1']]\n",
      "whelp [['D', 'AH1', 'B', 'AH0', 'L', 'Y', 'UW0', 'HH', 'EH1', 'L', 'P']]\n",
      "whil [['D', 'AH1', 'B', 'AH0', 'L', 'Y', 'UW0', 'HH', 'AY1', 'EH1', 'L']]\n",
      "whir [['D', 'AH1', 'B', 'AH0', 'L', 'Y', 'UW0', 'HH', 'AY1', 'AA1', 'R']]\n",
      "wretch [['D', 'AH1', 'B', 'AH0', 'L', 'Y', 'UW0', 'R', 'EH1', 'T', 'S', 'IY1', 'EY1', 'CH']]\n",
      "yawl [['Y', 'AA1', 'D', 'AH1', 'B', 'AH0', 'L', 'Y', 'UW0', 'EH1', 'L']]\n",
      "zit [['Z', 'IY1', 'T', 'IY1']]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product as iterprod\n",
    "#arpabet = nltk.corpus.cmudict.dict()\n",
    "\n",
    "def wordbreak(s):\n",
    "    \"\"\"\n",
    "    See https://stackoverflow.com/questions/33666557/get-phonemes-from-any-word-in-python-nltk-or-other-modules\n",
    "\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    if s in arpabet:\n",
    "        return arpabet[s]\n",
    "    middle = len(s)/2\n",
    "    partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
    "    for i in partition:\n",
    "        pre, suf = (s[:i], s[i:])\n",
    "        if pre in arpabet and wordbreak(suf) is not None:\n",
    "            return [x+y for x,y in iterprod(arpabet[pre], wordbreak(suf))]\n",
    "    return None\n",
    "\n",
    "# nltk の cmudict に存在しない単語の読みを表示\n",
    "for word in not_in_arpabet:\n",
    "    print(word, wordbreak(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 意味層の実装\n",
    "\n",
    "from <https://fasttext.cc/docs/en/pretrained-vectors.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastText データの読み込み。次の行頭の # を外してください\n",
    "#!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 2998)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_file = 'wiki.en.vec'\n",
    "with open(word2vec_file, 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 2998)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list = list(handson.grapheme)\n",
    "type(words_list), len(words_list)\n",
    "semantics_wiki = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 15s ± 809 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for line in lines:    \n",
    "    buff = line.strip().split(' ')\n",
    "    word = buff[0]\n",
    "    if word in words_list:\n",
    "        semantics_wiki[word] = np.asarray([float(x) for x in buff[1:]],dtype=np.float32)\n",
    "\n",
    "#同形異音語があるので，その数だけ semantics は数が少ないです。\n",
    "len(semantics_wiki), type(semantics_wiki)\n",
    "semantics = np.ndarray((len(words_list), 300), dtype=np.float32)\n",
    "# そこで数合わせをします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantics = np.ndarray((len(words_list), 300), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.48 ms ± 118 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for i, w in enumerate(words_list):\n",
    "    semantics[i] = semantics_wiki[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2998, numpy.ndarray, (2998,))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2998, numpy.ndarray, (2998,))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2998, numpy.ndarray, (2998, 300))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handson.sememe = semantics\n",
    "for modality in [handson.grapheme, handson.phoneme, handson.sememe]:\n",
    "    len(modality), type(modality), modality.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覚化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=4, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07402131 0.058128   0.05556856 0.04860908]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=4, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02843864 0.02434086 0.02053765 0.01836787]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=4, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05501097 0.04969663 0.0479385  0.0450488 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "G = handson.db['inputs']\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(G)\n",
    "print(pca.explained_variance_ratio_) \n",
    "\n",
    "S = handson.sememe\n",
    "pca.fit(S)\n",
    "print(pca.explained_variance_ratio_) \n",
    "\n",
    "P = handson.db['outputs']\n",
    "pca.fit(P)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True,\n",
       " 'iterated_power': 'auto',\n",
       " 'n_components': 2,\n",
       " 'random_state': None,\n",
       " 'svd_solver': 'auto',\n",
       " 'tol': 0.0,\n",
       " 'whiten': False}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handson.input.shape\n",
    "len(semantics)\n",
    "handson.grapheme[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in handson.grapheme[:3]:\n",
    "    print(word, semantics[word].shape, semantics[word].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(semantics)\n",
    "type(handson.grapheme), handson.grapheme.shape, handson.grapheme[:3]\n",
    "type(handson.phoneme), handson.phoneme.shape, handson.phoneme[:3]\n",
    "type(handson.sememe), handson.sememe.shape, handson.sememe[:3]\n",
    "type(handson.freq), handson.freq.shape, handson.freq[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 同形異音文字の表示\n",
    "<!--\n",
    "- [](https://en.wikipedia.org/wiki/File:Homograph_homophone_venn_diagram.svg)\n",
    "- [](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Homograph_homophone_venn_diagram.svg/1280px-Homograph_homophone_venn_diagram.svg.png)\n",
    "-->\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Homograph_homophone_venn_diagram.svg/1280px-Homograph_homophone_venn_diagram.svg.png\" style=\"width:29%\"><br>\n",
    "from [https://en.wikipedia.org/wiki/Heteronym_(linguistics)](https://en.wikipedia.org/wiki/Heteronym_(linguistics))<br>\n",
    "\n",
    "from <https://en.wikipedia.org/wiki/Heteronym_(linguistics)>\n",
    "</center>\n",
    "\n",
    "see also <http://www.singularis.ltd.uk/bifroest/misc/homophones-list.html>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = handson.graph # orthography\n",
    "prev_word = None\n",
    "same_count = 0\n",
    "for i, word in enumerate(words):\n",
    "    if not word in semantics:\n",
    "        print(i, word)  #, wrd2idx[word], semantics[word])\n",
    "        del orthography[word]\n",
    "        del phonology[word]\n",
    "    if prev_word == word:\n",
    "        same_count += 1\n",
    "        print(word, end=' ')\n",
    "    prev_word = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
